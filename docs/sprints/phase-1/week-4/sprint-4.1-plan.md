# Sprint 4.1 Plan: Docker & Deployment

**Sprint:** 4.1 - Docker & Deployment
**Phase:** 1 (Core MVP)
**Week:** 4
**Duration:** Days 22-24 (3 days)
**Status:** üü¢ READY TO START
**Started:** TBD
**Target Completion:** TBD

---

## Sprint Goal

Optimize SQL Sentinel's Docker deployment for production use with enhanced monitoring, health checks, and operational tooling. Transform the existing Docker setup from Sprint 2.2 into a production-ready deployment solution with comprehensive observability.

---

## Executive Summary

### Context from Sprint 3.2

Sprint 3.2 delivered BigQuery integration:
- ‚úÖ 412 tests passing (391 unit + 21 integration) with 89% coverage
- ‚úÖ Native BigQuery support via google-cloud-bigquery SDK
- ‚úÖ Comprehensive authentication (service account + ADC)
- ‚úÖ Cost awareness features (dry-run estimation)
- ‚úÖ Complete documentation (3 guides, 10 examples)
- ‚úÖ Zero breaking changes

**Current State:** SQL Sentinel supports multiple databases (SQLite, PostgreSQL, MySQL, BigQuery), has automated scheduling (daemon mode), multi-channel notifications (Email, Slack, Webhook), and basic Docker containerization from Sprint 2.2.

### Existing Docker Setup (from Sprint 2.2)

**Current Dockerfile:**
- ‚úÖ Multi-stage build (builder + runtime)
- ‚úÖ Python 3.11-slim base image
- ‚úÖ Poetry-based dependency management
- ‚úÖ Non-root user (sqlsentinel:sqlsentinel)
- ‚úÖ Basic environment configuration
- ‚ö†Ô∏è Basic health check (only checks if Python is alive)
- ‚ùì Image size unknown (needs measurement)
- ‚ùì Startup time unknown (needs measurement)

**Current docker-compose.yaml:**
- ‚úÖ Service definition with volumes and networking
- ‚úÖ Environment variable configuration
- ‚úÖ Restart policy (unless-stopped)
- ‚úÖ Volume mounts (config, state, logs)
- ‚ö†Ô∏è Single-scenario template (daemon mode only)
- ‚ùå No metrics collection
- ‚ùå No structured logging configuration

### Sprint 4.1 Objective

Transform the existing Docker setup into a **production-ready deployment solution**:

1. **Docker Image Optimization** - Measure, analyze, and optimize image size and startup time
2. **Health Monitoring** - Real health endpoints that validate application state
3. **Metrics Collection** - Prometheus-compatible metrics for observability
4. **Structured Logging** - JSON-formatted logs for aggregation and analysis
5. **Deployment Templates** - Multiple docker-compose scenarios (dev, prod, testing)
6. **Operational Tooling** - Scripts and documentation for common operations

---

## Scope & Deliverables

### In Scope ‚úÖ

1. **Docker Image Optimization**
   - Measure current image size and startup time
   - Optimize layer caching and image size
   - Validate <500MB target and <10s startup
   - Document image size breakdown
   - Pin dependency versions for reproducibility

2. **Health Check System**
   - Create `/health` HTTP endpoint (FastAPI or Flask)
   - Check database connectivity
   - Check scheduler status
   - Check notification services
   - Return structured JSON health status
   - Update Docker HEALTHCHECK to use endpoint

3. **Metrics Collection (Prometheus Format)**
   - Create `/metrics` HTTP endpoint
   - Expose Prometheus-compatible metrics:
     - Alert execution count/duration
     - Notification success/failure rates
     - Query execution times
     - Scheduler job count
     - System uptime
   - Document metrics in README

4. **Structured Logging**
   - Configure JSON logging format
   - Add contextual fields (alert_name, execution_id, timestamp)
   - Log levels configurable via environment variable
   - Correlation IDs for request tracing
   - Document log schema

5. **Deployment Templates**
   - `docker-compose.yaml` - Production daemon mode (existing, enhanced)
   - `docker-compose.dev.yaml` - Development mode with hot reload
   - `docker-compose.test.yaml` - Testing with sample database
   - `docker-compose.monitoring.yaml` - With Prometheus + Grafana
   - Environment file templates (`.env.example`)

6. **Operational Scripts**
   - `scripts/docker-build.sh` - Build and tag images
   - `scripts/docker-test.sh` - Test container health
   - `scripts/docker-push.sh` - Push to registry
   - Health check validation script
   - Log aggregation examples

7. **Documentation**
   - Docker deployment guide
   - Health check API reference
   - Metrics reference
   - Logging schema documentation
   - Troubleshooting guide
   - Production deployment checklist

### Out of Scope üö´

1. **Cloud-Specific Deployments** - No GCP/AWS/Azure deployment (Phase 2)
2. **Kubernetes/Helm** - Deferred to Phase 2/3
3. **CI/CD Pipeline** - Already exists, no changes needed
4. **Web UI** - Deferred to Phase 3
5. **Multi-Container Orchestration** - Single container focus for MVP
6. **Advanced Monitoring** - No Grafana dashboard creation (examples only)
7. **Log Aggregation Backend** - No ELK/Loki setup (examples only)

---

## Technical Architecture

### Health Check System

```
src/sqlsentinel/
‚îú‚îÄ‚îÄ health/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # NEW: Health module
‚îÇ   ‚îú‚îÄ‚îÄ server.py                # NEW: Lightweight HTTP server (Flask)
‚îÇ   ‚îî‚îÄ‚îÄ checks.py                # NEW: Health check functions
```

**Health Endpoint Design:**

```python
# GET /health
{
  "status": "healthy",  # healthy | degraded | unhealthy
  "timestamp": "2025-10-23T10:00:00Z",
  "version": "0.1.0",
  "checks": {
    "database": {
      "status": "healthy",
      "latency_ms": 12.3,
      "message": "Connected to BigQuery"
    },
    "scheduler": {
      "status": "healthy",
      "jobs_count": 5,
      "next_run": "2025-10-23T10:05:00Z"
    },
    "notifications": {
      "email": {"status": "healthy"},
      "slack": {"status": "healthy"},
      "webhook": {"status": "degraded", "message": "High latency"}
    }
  },
  "uptime_seconds": 3600
}
```

**Metrics Endpoint Design:**

```python
# GET /metrics (Prometheus format)
# HELP sqlsentinel_alerts_total Total number of alert executions
# TYPE sqlsentinel_alerts_total counter
sqlsentinel_alerts_total{alert_name="revenue_check",status="ok"} 150
sqlsentinel_alerts_total{alert_name="revenue_check",status="alert"} 5

# HELP sqlsentinel_alert_duration_seconds Alert execution duration
# TYPE sqlsentinel_alert_duration_seconds histogram
sqlsentinel_alert_duration_seconds_bucket{alert_name="revenue_check",le="0.1"} 10
sqlsentinel_alert_duration_seconds_bucket{alert_name="revenue_check",le="0.5"} 50
sqlsentinel_alert_duration_seconds_sum{alert_name="revenue_check"} 75.5
sqlsentinel_alert_duration_seconds_count{alert_name="revenue_check"} 155

# HELP sqlsentinel_notifications_total Total notifications sent
# TYPE sqlsentinel_notifications_total counter
sqlsentinel_notifications_total{channel="email",status="success"} 50
sqlsentinel_notifications_total{channel="email",status="failure"} 2

# HELP sqlsentinel_scheduler_jobs Current number of scheduled jobs
# TYPE sqlsentinel_scheduler_jobs gauge
sqlsentinel_scheduler_jobs 5

# HELP sqlsentinel_uptime_seconds Application uptime in seconds
# TYPE sqlsentinel_uptime_seconds counter
sqlsentinel_uptime_seconds 3600
```

### Structured Logging Format

```json
{
  "timestamp": "2025-10-23T10:00:00.123Z",
  "level": "INFO",
  "logger": "sqlsentinel.executor.alert_executor",
  "message": "Alert execution completed",
  "context": {
    "alert_name": "revenue_check",
    "execution_id": "550e8400-e29b-41d4-a716-446655440000",
    "status": "ALERT",
    "duration_ms": 123.45,
    "actual_value": 8500,
    "threshold": 10000
  },
  "correlation_id": "req-abc123",
  "environment": "production"
}
```

### Docker Compose Architecture

```
docker/
‚îú‚îÄ‚îÄ docker-compose.yaml           # Production daemon mode
‚îú‚îÄ‚îÄ docker-compose.dev.yaml       # Development with hot reload
‚îú‚îÄ‚îÄ docker-compose.test.yaml      # Testing with sample DB
‚îú‚îÄ‚îÄ docker-compose.monitoring.yaml # With Prometheus + Grafana
‚îî‚îÄ‚îÄ .env.example                  # Environment template
```

---

## Implementation Plan

### Phase 1: Health Check System (Day 22 - Morning)

**Goal:** Create health check endpoint and update Docker HEALTHCHECK

**Tasks:**
1. Create `src/sqlsentinel/health/` module
2. Implement lightweight HTTP server (Flask or FastAPI)
3. Create health check functions:
   - Database connectivity check
   - Scheduler status check
   - Basic system checks (memory, disk)
4. Create `/health` endpoint returning JSON
5. Update Dockerfile HEALTHCHECK to use endpoint
6. Write tests for health checks

**Deliverables:**
- [ ] Health module implementation (100 lines)
- [ ] Flask/FastAPI lightweight server (50 lines)
- [ ] Health check functions (150 lines)
- [ ] Updated Dockerfile with better HEALTHCHECK
- [ ] 15+ unit tests for health checks

**Success Criteria:**
- Health endpoint responds in <100ms
- Correctly detects database issues
- Correctly detects scheduler issues
- Docker HEALTHCHECK passes/fails appropriately
- Tests cover all health check scenarios

**Files to Create:**
```
src/sqlsentinel/health/__init__.py
src/sqlsentinel/health/server.py
src/sqlsentinel/health/checks.py
tests/health/test_server.py
tests/health/test_checks.py
```

**Files to Modify:**
```
Dockerfile (update HEALTHCHECK)
docker-compose.yaml (update health check)
pyproject.toml (add Flask dependency)
```

---

### Phase 2: Metrics Collection (Day 22 - Afternoon)

**Goal:** Add Prometheus metrics endpoint and instrumentation

**Tasks:**
1. Add `prometheus_client` dependency
2. Create metrics collection module
3. Define metrics (counters, histograms, gauges)
4. Instrument AlertExecutor with metrics
5. Instrument Scheduler with metrics
6. Instrument notification services with metrics
7. Create `/metrics` endpoint
8. Write tests for metrics collection

**Deliverables:**
- [ ] Metrics module implementation (200 lines)
- [ ] AlertExecutor instrumentation
- [ ] Scheduler instrumentation
- [ ] Notification instrumentation
- [ ] `/metrics` endpoint
- [ ] 20+ unit tests for metrics

**Success Criteria:**
- Metrics endpoint returns Prometheus format
- All key operations tracked
- Metrics accurate (match execution history)
- Labels used correctly (alert_name, channel, status)
- Documentation explains each metric

**Files to Create:**
```
src/sqlsentinel/metrics/__init__.py
src/sqlsentinel/metrics/collector.py
src/sqlsentinel/metrics/registry.py
tests/metrics/test_collector.py
tests/metrics/test_integration.py
```

**Files to Modify:**
```
src/sqlsentinel/executor/alert_executor.py (add metrics)
src/sqlsentinel/scheduler/scheduler.py (add metrics)
src/sqlsentinel/notifications/email.py (add metrics)
src/sqlsentinel/notifications/slack.py (add metrics)
src/sqlsentinel/notifications/webhook.py (add metrics)
src/sqlsentinel/health/server.py (add /metrics route)
pyproject.toml (add prometheus_client)
```

---

### Phase 3: Structured Logging (Day 23 - Morning)

**Goal:** Implement JSON-formatted structured logging

**Tasks:**
1. Create logging configuration module
2. Implement JSON log formatter
3. Add contextual fields (execution_id, alert_name)
4. Add correlation ID support
5. Update all modules to use structured logging
6. Add log level configuration via environment
7. Write tests for logging

**Deliverables:**
- [ ] Logging configuration module (100 lines)
- [ ] JSON formatter implementation (80 lines)
- [ ] Updated modules with structured logging
- [ ] Environment variable configuration
- [ ] 12+ unit tests for logging

**Success Criteria:**
- All logs output valid JSON
- Contextual fields present in all log entries
- Log level configurable (DEBUG, INFO, WARN, ERROR)
- Correlation IDs work for request tracing
- Human-readable fallback for development

**Files to Create:**
```
src/sqlsentinel/logging/__init__.py
src/sqlsentinel/logging/formatter.py
src/sqlsentinel/logging/config.py
tests/logging/test_formatter.py
tests/logging/test_config.py
```

**Files to Modify:**
```
src/sqlsentinel/cli.py (configure logging)
src/sqlsentinel/executor/alert_executor.py (use structured logging)
src/sqlsentinel/scheduler/scheduler.py (use structured logging)
src/sqlsentinel/notifications/*.py (use structured logging)
Dockerfile (set LOG_FORMAT env var)
docker-compose.yaml (configure log driver)
```

---

### Phase 4: Docker Image Optimization (Day 23 - Afternoon)

**Goal:** Measure and optimize Docker image

**Tasks:**
1. Measure current image size (docker images)
2. Measure current startup time (docker run + time to health)
3. Analyze image layers (docker history)
4. Optimize if needed:
   - Remove unnecessary dependencies
   - Optimize layer caching
   - Multi-stage build improvements
   - Use .dockerignore
5. Pin all dependency versions
6. Document image size breakdown
7. Test optimized image

**Deliverables:**
- [ ] Image size measurement script
- [ ] Startup time benchmark script
- [ ] Optimized Dockerfile (if needed)
- [ ] `.dockerignore` file
- [ ] Size analysis documentation

**Success Criteria:**
- Image size <500MB (target: <300MB)
- Startup time <10 seconds (target: <5s)
- Build time optimized with caching
- All layers documented
- Reproducible builds (pinned versions)

**Files to Create:**
```
scripts/docker-measure.sh
scripts/docker-benchmark.sh
.dockerignore
docs/docker/image-optimization.md
```

**Files to Modify:**
```
Dockerfile (optimizations)
pyproject.toml (pin versions)
```

---

### Phase 5: Deployment Templates (Day 24 - Morning)

**Goal:** Create docker-compose templates for different scenarios

**Tasks:**
1. Enhance production docker-compose.yaml
2. Create docker-compose.dev.yaml (hot reload, verbose logging)
3. Create docker-compose.test.yaml (with PostgreSQL)
4. Create docker-compose.monitoring.yaml (Prometheus + Grafana)
5. Create .env.example with all variables
6. Write deployment scripts (build, test, push)
7. Test all templates

**Deliverables:**
- [ ] Enhanced docker-compose.yaml
- [ ] docker-compose.dev.yaml
- [ ] docker-compose.test.yaml
- [ ] docker-compose.monitoring.yaml
- [ ] .env.example
- [ ] deployment scripts (3 scripts)

**Success Criteria:**
- All compose files work correctly
- Development mode supports hot reload
- Test mode includes sample database
- Monitoring stack works (Prometheus + Grafana)
- Scripts automate common operations
- Documentation explains each template

**Files to Create:**
```
docker-compose.dev.yaml
docker-compose.test.yaml
docker-compose.monitoring.yaml
.env.example
scripts/docker-build.sh
scripts/docker-test.sh
scripts/docker-push.sh
```

**Files to Modify:**
```
docker-compose.yaml (enhance production template)
```

---

### Phase 6: Operational Tooling (Day 24 - Afternoon - Part 1)

**Goal:** Create operational scripts and utilities

**Tasks:**
1. Create health check validation script
2. Create log aggregation examples
3. Create backup/restore scripts (for state DB)
4. Create debugging helper script
5. Create performance monitoring script
6. Write operational playbook

**Deliverables:**
- [ ] Health validation script
- [ ] Log aggregation examples
- [ ] Backup/restore scripts
- [ ] Debug helper script
- [ ] Performance monitor script
- [ ] Operational playbook

**Success Criteria:**
- Scripts work reliably
- Clear error messages
- Examples documented
- Playbook covers common scenarios
- Scripts tested on all compose templates

**Files to Create:**
```
scripts/validate-health.sh
scripts/aggregate-logs.sh
scripts/backup-state.sh
scripts/restore-state.sh
scripts/debug-container.sh
scripts/monitor-performance.sh
docs/operations/playbook.md
```

---

### Phase 7: Documentation (Day 24 - Afternoon - Part 2)

**Goal:** Complete deployment and operations documentation

**Tasks:**
1. Write Docker deployment guide
2. Document health check API
3. Document metrics reference
4. Document logging schema
5. Write troubleshooting guide
6. Create production deployment checklist
7. Write sprint completion report

**Deliverables:**
- [ ] docs/deployment/docker-guide.md
- [ ] docs/api/health-checks.md
- [ ] docs/api/metrics.md
- [ ] docs/operations/logging-schema.md
- [ ] docs/operations/troubleshooting.md
- [ ] docs/deployment/production-checklist.md
- [ ] docs/sprints/phase-1/week-4/sprint-4.1-completion.md

**Success Criteria:**
- Deployment guide enables user success
- API documentation complete with examples
- Metrics fully documented
- Logging schema clear
- Troubleshooting covers common issues
- Production checklist comprehensive

**Files to Create:**
```
docs/deployment/docker-guide.md
docs/api/health-checks.md
docs/api/metrics.md
docs/operations/logging-schema.md
docs/operations/troubleshooting-docker.md
docs/deployment/production-checklist.md
docs/sprints/phase-1/week-4/sprint-4.1-completion.md
```

**Files to Modify:**
```
README.md (add deployment section)
```

---

### Phase 8: Testing & Quality (Day 24 - End of Day)

**Goal:** Comprehensive testing and validation

**Tasks:**
1. Run full test suite (412+ existing tests)
2. Test all new health/metrics/logging features
3. Test all docker-compose templates
4. Validate image size and startup time
5. Run linting and type checking
6. Integration test with monitoring stack
7. Performance validation

**Deliverables:**
- [ ] All 450+ tests passing
- [ ] >85% coverage maintained
- [ ] All docker-compose templates validated
- [ ] Performance benchmarks documented
- [ ] Quality gates passed

**Success Criteria:**
- All tests passing (412 existing + 40 new)
- >85% overall coverage maintained
- Image size <500MB (ideally <300MB)
- Startup time <10s (ideally <5s)
- All linting checks pass
- No regressions

---

## Dependencies & Risks

### Dependencies

**New Python Packages:**
1. **Flask** (^3.0) - Lightweight HTTP server for health/metrics
   - Minimal overhead
   - Well-tested and stable
   - Easy to containerize
   - Alternative: FastAPI (heavier but more features)

2. **prometheus_client** (^0.19) - Prometheus metrics
   - Official Prometheus client library
   - Supports counters, gauges, histograms
   - Widely used in production

3. **python-json-logger** (^2.0) - JSON log formatting
   - Structured logging support
   - Compatible with all log aggregators
   - Minimal overhead

**Infrastructure:**
- Docker 20.10+ (for multi-stage builds)
- Docker Compose 2.0+ (for compose files)
- (Optional) Prometheus for metrics collection
- (Optional) Grafana for visualization

### Risks & Mitigation

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| **Health endpoint adds latency** | Medium | Low | Lightweight Flask, minimal checks, async if needed |
| **Metrics overhead** | Medium | Low | Use counters/gauges, avoid expensive operations |
| **JSON logging breaks compatibility** | High | Low | Support both JSON and text formats via env var |
| **Image size exceeds 500MB** | High | Low | Already using slim image, measure first, optimize if needed |
| **Startup time too slow** | Medium | Low | Profile startup, optimize if needed |
| **Docker compose complexity** | Low | Medium | Clear documentation, examples for each template |

---

## Testing Strategy

### Unit Tests (40+ new tests)

**Health Checks (tests/health/test_checks.py):**
```python
class TestHealthChecks:
    def test_database_health_check_healthy(self):
        """Test database health check when DB is available."""

    def test_database_health_check_unhealthy(self):
        """Test database health check when DB is unavailable."""

    def test_scheduler_health_check(self):
        """Test scheduler health check."""

    def test_overall_health_status(self):
        """Test aggregated health status."""
```

**Metrics (tests/metrics/test_collector.py):**
```python
class TestMetricsCollector:
    def test_alert_execution_counter(self):
        """Test alert execution counter increments."""

    def test_alert_duration_histogram(self):
        """Test duration histogram records values."""

    def test_notification_counter(self):
        """Test notification counter per channel."""

    def test_prometheus_format_export(self):
        """Test metrics export in Prometheus format."""
```

**Logging (tests/logging/test_formatter.py):**
```python
class TestJSONFormatter:
    def test_json_log_format(self):
        """Test logs output valid JSON."""

    def test_contextual_fields(self):
        """Test contextual fields included."""

    def test_correlation_id(self):
        """Test correlation ID propagation."""
```

### Integration Tests (10+ new tests)

**Docker Integration (tests/integration/test_docker.py):**
```python
@pytest.mark.integration
class TestDockerDeployment:
    def test_container_starts(self):
        """Test container starts successfully."""

    def test_health_endpoint_accessible(self):
        """Test health endpoint responds."""

    def test_metrics_endpoint_accessible(self):
        """Test metrics endpoint responds."""

    def test_structured_logs_output(self):
        """Test logs are JSON formatted."""

    def test_daemon_mode_works(self):
        """Test daemon mode executes alerts."""
```

### Performance Tests (5+ tests)

**Benchmark (tests/performance/test_benchmarks.py):**
```python
class TestPerformance:
    def test_startup_time(self):
        """Test container starts in <10 seconds."""

    def test_health_endpoint_latency(self):
        """Test health endpoint responds in <100ms."""

    def test_metrics_endpoint_latency(self):
        """Test metrics endpoint responds in <200ms."""

    def test_image_size(self):
        """Test image size is <500MB."""
```

---

## Success Criteria

### Feature Completeness ‚úÖ

- [ ] Health check endpoint working (/health)
- [ ] Metrics endpoint working (/metrics)
- [ ] Structured JSON logging implemented
- [ ] All docker-compose templates functional
- [ ] Operational scripts working
- [ ] Documentation complete

### Quality Metrics ‚úÖ

- [ ] 40+ new tests added (450+ total)
- [ ] >85% overall coverage maintained
- [ ] All linting checks passing (Black, Ruff, mypy)
- [ ] No regressions in existing functionality

### Performance Metrics ‚úÖ

- [ ] Docker image size <500MB (target: <300MB)
- [ ] Container startup time <10s (target: <5s)
- [ ] Health endpoint latency <100ms
- [ ] Metrics endpoint latency <200ms

### Operational Readiness ‚úÖ

- [ ] Production deployment checklist complete
- [ ] Troubleshooting guide covers common issues
- [ ] All templates tested and working
- [ ] Scripts automate common operations
- [ ] Monitoring stack example working

---

## Timeline

| Day | Phase | Focus | Key Deliverables |
|-----|-------|-------|------------------|
| **Day 22** | 1-2 | Health + Metrics | Health endpoint, metrics endpoint, 35 tests |
| **Day 23** | 3-4 | Logging + Optimization | Structured logging, optimized image, benchmarks |
| **Day 24** | 5-8 | Templates + Docs | Compose templates, scripts, documentation, completion report |

---

## Examples & Use Cases

### Example 1: Production Deployment with Monitoring

```bash
# Clone repository
git clone https://github.com/yourorg/sqlsentinel.git
cd sqlsentinel

# Copy environment template
cp .env.example .env

# Edit configuration
nano .env  # Set DATABASE_URL, SMTP credentials, etc.

# Deploy with monitoring stack
docker-compose -f docker-compose.yaml -f docker-compose.monitoring.yaml up -d

# Verify health
curl http://localhost:8080/health

# View metrics
curl http://localhost:8080/metrics

# Access Grafana
open http://localhost:3000  # admin/admin
```

### Example 2: Development Mode with Hot Reload

```bash
# Use development compose file
docker-compose -f docker-compose.dev.yaml up

# Logs show detailed debugging
# Changes to Python files auto-reload
# Verbose logging enabled
```

### Example 3: Testing with Sample Database

```bash
# Start with test template (includes PostgreSQL)
docker-compose -f docker-compose.test.yaml up -d

# Run sample alerts
docker-compose exec sqlsentinel sqlsentinel run /app/config/test-alerts.yaml

# View execution history
docker-compose exec sqlsentinel sqlsentinel history /app/config/test-alerts.yaml
```

### Example 4: Health Check Validation

```bash
# Check container health
./scripts/validate-health.sh

# Output:
# ‚úì Container is running
# ‚úì Health endpoint accessible
# ‚úì Database connectivity: OK
# ‚úì Scheduler status: 5 jobs scheduled
# ‚úì Email notifications: OK
# ‚úì Slack notifications: OK
# ‚úì Overall status: HEALTHY
```

### Example 5: Metrics Collection

```bash
# View live metrics
watch -n 5 'curl -s http://localhost:8080/metrics | grep sqlsentinel'

# Example output:
# sqlsentinel_alerts_total{alert_name="revenue_check",status="ok"} 150
# sqlsentinel_alert_duration_seconds_sum{alert_name="revenue_check"} 75.5
# sqlsentinel_notifications_total{channel="email",status="success"} 50
```

---

## Definition of Done

Sprint 4.1 is complete when:

1. ‚úÖ All 8 phases delivered
2. ‚úÖ 450+ tests passing (412 existing + 40 new)
3. ‚úÖ >85% overall code coverage maintained
4. ‚úÖ All linting checks passing
5. ‚úÖ Health endpoint functional and tested
6. ‚úÖ Metrics endpoint functional and tested
7. ‚úÖ Structured logging implemented
8. ‚úÖ Image size <500MB, startup <10s
9. ‚úÖ All docker-compose templates working
10. ‚úÖ Complete documentation delivered
11. ‚úÖ Sprint completion report written
12. ‚úÖ No breaking changes to existing functionality

---

## Post-Sprint Activities

### Immediate Follow-up (Sprint 4.2)

**Sprint 4.2: MVP Testing & Documentation** (per roadmap)
- Comprehensive end-to-end testing
- Performance testing framework
- Security validation
- Complete documentation set
- MVP demo environment
- Performance benchmarks

### Phase 1 Completion

After Sprint 4.1, Phase 1 (Core MVP) will have:
- ‚úÖ Core alert execution engine
- ‚úÖ Multi-channel notifications (Email, Slack, Webhook)
- ‚úÖ Automated scheduling (daemon mode)
- ‚úÖ BigQuery support (first cloud warehouse!)
- ‚úÖ **Production-ready Docker deployment**
- ‚úÖ **Health monitoring and metrics**
- ‚úÖ **Structured logging**
- ‚úÖ Comprehensive documentation

---

## Next Steps

### Immediate Actions

1. **Review this plan** - Team/stakeholder approval
2. **Create feature branch** - `sprint-4.1-docker-deployment`
3. **Add dependencies** - Flask, prometheus_client, python-json-logger
4. **Create directory structure** - health/, metrics/, logging/ modules
5. **Start Phase 1** - Health check system implementation

### Future Enhancements (Phase 2+)

1. **Cloud Deployments** - GCP Cloud Run, AWS ECS, Azure Container Instances
2. **Kubernetes/Helm** - K8s manifests and Helm charts
3. **Advanced Monitoring** - Pre-built Grafana dashboards
4. **Distributed Tracing** - OpenTelemetry integration
5. **Advanced Logging** - Log aggregation (ELK, Loki)

---

## References

- [IMPLEMENTATION_ROADMAP.md](../../../IMPLEMENTATION_ROADMAP.md) - Overall project plan
- [Sprint 3.2 Completion](../week-3/sprint-3.2-completion.md) - Previous sprint results
- [Sprint 2.2 Completion](../week-2/sprint-2.2-completion.md) - Initial Docker implementation
- [Docker Best Practices](https://docs.docker.com/develop/dev-best-practices/) - Official docs
- [Prometheus Python Client](https://github.com/prometheus/client_python) - Metrics library
- [Flask Documentation](https://flask.palletsprojects.com/) - HTTP server
- [Twelve-Factor App](https://12factor.net/) - Application architecture principles

---

**Prepared by:** Claude (AI Assistant)
**Date:** 2025-10-26
**Sprint:** 4.1 - Docker & Deployment
**Status:** üü¢ READY TO START
